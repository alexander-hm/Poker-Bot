{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import collections\n",
    "from typing import Iterable\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypokerengine.players import BasePokerPlayer\n",
    "from pypokerengine.utils.card_utils import estimate_hole_card_win_rate, gen_cards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Program settings\n",
    "global DEBUG\n",
    "DEBUG = False\n",
    "global WATCH_GAME\n",
    "\n",
    "\n",
    "# Load saved model or create new\n",
    "global save_model_to_file\n",
    "save_model_to_file = True\n",
    "global load_saved_model\n",
    "load_saved_model = True\n",
    "model_file_name = 'Daniel_Negreanu_FINAL'\n",
    "\n",
    "# Model info\n",
    "model_input_size = 21\n",
    "model_output_size = 4\n",
    "MEMORY_SIZE = 10000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "#Training Info\n",
    "training_mode_on = True\n",
    "NUM_EPISODES = 1000\n",
    "TARGET_LAG_FACTOR = 7500\n",
    "INITIAL_STACK = 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep RL constrants\n",
    "gamma = 0.85 \n",
    "\n",
    "\n",
    "# Greedy policy\n",
    "# Probability of choosing any action at random (vs. action with highest Q value)\n",
    "epsilon = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_QNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, 64)\n",
    "        self.fc2 = torch.nn.Linear(64, 64)\n",
    "        self.fc3 = torch.nn.Linear(64, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "    def save(self, file_name='model.pth'):\n",
    "        torch.save(self.state_dict(), file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNetwork:\n",
    "    def __init__(self):\n",
    "        # model\n",
    "        self.model = Linear_QNet(model_input_size, model_output_size)\n",
    "        if load_saved_model:\n",
    "            print('Loading model...:' + model_file_name)\n",
    "            model_state = torch.load(\"./models/\" + model_file_name)\n",
    "            self.model.load_state_dict(model_state)               \n",
    "        # set optimizer and loss functions for models\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def get_NN(self):\n",
    "        return self.model\n",
    "    \n",
    "    def train_step(self, state, action, reward, next_state, done):\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "        action = torch.tensor(action, dtype=torch.float32)\n",
    "        \n",
    "        \n",
    "        # get model's predicted Q values at the state and make copy\n",
    "        q_values = self.model.forward(state)\n",
    "        targ = q_values.clone()\n",
    "\n",
    "\n",
    "        # apply Bellman equation to get target q value for this action at this state\n",
    "        updated_Q = reward\n",
    "            \n",
    "        if not done:\n",
    "            updated_Q += gamma * torch.max(self.model(next_state))\n",
    "\n",
    "        # get the index of the action take at the state\n",
    "        action_idx = torch.argmax(action).item()\n",
    "\n",
    "        #set target q value for action at state to calculated\n",
    "\n",
    "        targ[0][action_idx] = updated_Q \n",
    "\n",
    "        # train with predicted Q-values vs target\n",
    "\n",
    "        loss = self.criterion(targ, q_values)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper methods\n",
    "def flatten(x):\n",
    "    if isinstance(x, Iterable):\n",
    "        return [a for i in x for a in flatten(i)]\n",
    "    else:\n",
    "        return [x]      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PokerAgent(BasePokerPlayer):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Initialize model\n",
    "        self.model = DQNetwork()       \n",
    "\n",
    "        # Experience replay\n",
    "        self.memory = collections.deque(maxlen = MEMORY_SIZE)\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.first_move = True\n",
    "        self.last_action = [0] * model_output_size\n",
    "        self.last_state = [0] * model_input_size\n",
    "        self.curr_stack = INITIAL_STACK\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            return random.randint(0, model_output_size - 1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.tensor(state, dtype=torch.float32).view(1, -1)\n",
    "                q_values = self.model.get_NN().forward(state)\n",
    "                return torch.argmax(q_values).item()\n",
    "        \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def train_short_mem(self, state, action, reward, next_state, done):\n",
    "        self.model.train_step(state, action, reward, next_state, done)\n",
    "\n",
    "    def train_long_mem(self):\n",
    "        \n",
    "        sample = None\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            sample = random.sample(self.memory, BATCH_SIZE)\n",
    "        else:\n",
    "            sample = self.memory\n",
    "            \n",
    "        for state, action, reward, next_state, done in sample:\n",
    "            self.model.train_step(state, action, reward, next_state, done)\n",
    "       \n",
    "\n",
    "    def declare_action(self, valid_actions, hole_card, round_state):\n",
    "        # Prepare feature vector based on the game state\n",
    "        feature_vector = self._extract_features(hole_card, round_state)\n",
    "\n",
    "        if not self.first_move and training_mode_on:\n",
    "            self.train_short_mem(self.last_state, self.last_action, 0, feature_vector, False)\n",
    "            self.remember(self.last_state, self.last_action, 0, feature_vector, False)\n",
    "\n",
    "        self.first_move = False\n",
    "\n",
    "        if DEBUG:\n",
    "            print(\"input size: \" + str(len(feature_vector)))\n",
    "            print(\"input shape: \" + str(feature_vector.shape))\n",
    "        \n",
    "        \n",
    "        # Use the model to predict the action\n",
    "        # action will be number 1-4\n",
    "        ## 1 -> fold\n",
    "        ## 2 -> call\n",
    "        ## 3 -> min raise\n",
    "        ## 4 -> max raise\n",
    "        action_num = self.select_action(feature_vector)\n",
    "\n",
    "        action_map = {0: 'fold', 1: 'call', 2: 'raise', 3: 'raise'}\n",
    "\n",
    "        action = action_map.get(action_num, 0)\n",
    "        amount = 0\n",
    "\n",
    "        #get call val\n",
    "        if action_num == 1:\n",
    "            amount = valid_actions[1]['amount']\n",
    "\n",
    "        # get min raise val\n",
    "        if action_num == 2:\n",
    "            amount = valid_actions[2]['amount']['min']\n",
    "        \n",
    "        # get max raise val\n",
    "        if action_num == 3:\n",
    "            amount = valid_actions[2]['amount']['min']\n",
    "        \n",
    "        self.last_action = [0, 0, 0, 0]\n",
    "        self.last_action[action_num] = 1\n",
    "        self.last_state = feature_vector\n",
    "        \n",
    "        if amount == -1:\n",
    "            action = 'call'\n",
    "            amount = valid_actions[1]['amount']\n",
    "        return action, amount\n",
    "    \n",
    "    def receive_game_start_message(self, game_info):\n",
    "        \n",
    "        self.roudstart_stack = INITIAL_STACK\n",
    "\n",
    "    \n",
    "    def _extract_features(self, hole_card, round_state, win = None):\n",
    "\n",
    "        \n",
    "        #simulate hand against 10000 flops extracting hand strength estimate, unless round over\n",
    "        hand_strength = 0\n",
    "        if win != None:\n",
    "            if win:\n",
    "                hand_strength = 1000\n",
    "            else:\n",
    "                hand_strength = 0\n",
    "        else:\n",
    "            hand_strength = estimate_hole_card_win_rate(1000, 3, gen_cards(hole_card), gen_cards(round_state['community_card']))\n",
    "\n",
    "        # 8 Standard features\n",
    "        \n",
    "        standard_features = [\n",
    "            round_state['round_count'],\n",
    "            round_state['pot']['main']['amount'],\n",
    "            sum([side_pot['amount'] for side_pot in round_state['pot']['side']]),\n",
    "            round_state['dealer_btn'],\n",
    "            round_state['small_blind_pos'],\n",
    "            round_state['big_blind_pos'],\n",
    "            round_state['small_blind_amount'],\n",
    "            self._street_to_feature(round_state['street'])\n",
    "        ]\n",
    "\n",
    "        # 8 Action history features (2 {# raises, # calls} for each betting stage: preflop, flop, turn, river)\n",
    "        action_history_features = self._aggregate_action_histories(round_state['action_histories'])\n",
    "\n",
    "        # Combine all features into a single fixed-size feature vector of length 34\n",
    "        # Flatten the list of lists\n",
    "        features = flatten([hand_strength] + standard_features + action_history_features)\n",
    "        features = np.array(features)\n",
    "        features = features.reshape(1, -1)\n",
    "        return features\n",
    "    \n",
    "    # Not neccesarily useful\n",
    "    def receive_round_start_message(self, round_count, hole_card, seats):\n",
    "        for seat in seats:\n",
    "            if seat['uuid']==self.uuid:\n",
    "                self.roudstart_stack = seat['stack']\n",
    "        self.first_move = True\n",
    "\n",
    "\n",
    "    # Not neccesarily useful\n",
    "    def receive_street_start_message(self, street, round_state):\n",
    "        pass\n",
    "\n",
    "    def _street_to_feature(self, street):\n",
    "        # Convert street to a numerical feature\n",
    "        streets = {'preflop': 1, 'flop': 2, 'turn': 3, 'river': 4, 'showdown': 5}\n",
    "        return streets.get(street, 0)\n",
    "    \n",
    "\n",
    "\n",
    "    def _aggregate_action_histories(self, action_histories):\n",
    "        '''\n",
    "        # Aggregate action histories into a fixed-length vector\n",
    "        # Example: Count the number of raises, calls, etc.\n",
    "        raise_count = sum(1 for action in action_histories.get('preflop', []) if action['action'] == 'raise')\n",
    "        call_count = sum(1 for action in action_histories.get('preflop', []) if action['action'] == 'call')\n",
    "        # Add more aggregated features as needed\n",
    "        # Ensure the length of this vector is fixed\n",
    "        return [raise_count, call_count]\n",
    "        '''\n",
    "        \n",
    "        # Initialize counts\n",
    "        raise_count = [0, 0, 0, 0]  # Preflop, Flop, Turn, River\n",
    "        call_count = [0, 0, 0, 0]\n",
    "        fold_count = [0, 0, 0, 0]\n",
    "\n",
    "        # Define rounds\n",
    "        rounds = ['preflop', 'flop', 'turn', 'river']\n",
    "\n",
    "        # Count actions in each round\n",
    "        for i, round in enumerate(rounds):\n",
    "            for action in action_histories.get(round, []):\n",
    "                if action['action'] == 'raise':\n",
    "                    raise_count[i] += 1\n",
    "                elif action['action'] == 'call':\n",
    "                    call_count[i] += 1\n",
    "                elif action['action'] == 'fold':\n",
    "                    fold_count[i] += 1\n",
    "\n",
    "        # Flatten and return\n",
    "        return raise_count + call_count + fold_count\n",
    "\n",
    "    # Can incorporate player observation in model updated with each move\n",
    "    def receive_game_update_message(self, new_action, round_state):\n",
    "        pass\n",
    "    \n",
    "    def receive_round_result_message(self, winners, hand_info, round_state):\n",
    "        # Calculate net chip gain from round\n",
    "        reward = 0\n",
    "        win = False\n",
    "        for w in winners:\n",
    "            if w['uuid'] == self.uuid:\n",
    "                win = True\n",
    "        for player in round_state['seats']:\n",
    "            if player['uuid'] == self.uuid:\n",
    "                new_stack = player['stack']\n",
    "                reward = 1.75 * (new_stack - self.curr_stack)\n",
    "                self.curr_stack = new_stack\n",
    "        \n",
    "        final_state = self._extract_features(None, round_state, win)\n",
    "\n",
    "        if training_mode_on:\n",
    "            #train model with reward as net chip gain\n",
    "            self.train_short_mem(self.last_state, self.last_action, reward, final_state, True)        \n",
    "            self.remember(self.last_state, self.last_action, reward, final_state, True) \n",
    "\n",
    "    def save_agent(self, file_name):\n",
    "        if save_model_to_file:\n",
    "            self.model.get_NN().save(file_name)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MAKES RANDOM MOVE EVERY TURN, EVENLY DISTRIBUTED\n",
    "\n",
    "class RandomPlayer(BasePokerPlayer): \n",
    "\n",
    "    def declare_action(self, valid_actions, hole_card, round_state):\n",
    "        # valid_actions format => [raise_action_info, call_action_info, fold_action_info]\n",
    "        index = random.choice(valid_actions)\n",
    "        action = index['action']\n",
    "        amount = index['amount']\n",
    "        if action == 'raise':\n",
    "            choice = random.choice(['min', 'max'])\n",
    "            amount = index['amount'][choice]\n",
    "        return action, amount   # action returned here is sent to the poker engine\n",
    "\n",
    "    def receive_game_start_message(self, game_info):\n",
    "        pass\n",
    "\n",
    "    def receive_round_start_message(self, round_count, hole_card, seats):\n",
    "        pass\n",
    "\n",
    "    def receive_street_start_message(self, street, round_state):\n",
    "        pass\n",
    "\n",
    "    def receive_game_update_message(self, action, round_state):\n",
    "        pass\n",
    "\n",
    "    def receive_round_result_message(self, winners, hand_info, round_state):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAKES PURELY RATIONAL MOVE EVERY TURN, PROPORTIONAL TO WIN RATE\n",
    "class HonestPlayer(BasePokerPlayer):\n",
    "\n",
    "    def declare_action(self, valid_actions, hole_card, round_state):\n",
    "        community_card = round_state['community_card']\n",
    "        win_rate = estimate_hole_card_win_rate(\n",
    "                nb_simulation=1000,\n",
    "                nb_player=self.nb_player,\n",
    "                hole_card=gen_cards(hole_card),\n",
    "                community_card=gen_cards(community_card)\n",
    "                )\n",
    "        if win_rate >= 1.0 / self.nb_player:\n",
    "            action = valid_actions[1]  # fetch CALL action info\n",
    "        else:\n",
    "            action = valid_actions[0]  # fetch FOLD action info\n",
    "        return action['action'], action['amount']\n",
    "\n",
    "    def receive_game_start_message(self, game_info):\n",
    "        self.nb_player = game_info['player_num']\n",
    "\n",
    "    def receive_round_start_message(self, round_count, hole_card, seats):\n",
    "        pass\n",
    "\n",
    "    def receive_street_start_message(self, street, round_state):\n",
    "        pass\n",
    "\n",
    "    def receive_game_update_message(self, action, round_state):\n",
    "        pass\n",
    "\n",
    "    def receive_round_result_message(self, winners, hand_info, round_state):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypokerengine.api.game import setup_config, start_poker\n",
    "\n",
    "#create models to train\n",
    "\n",
    "player1 = PokerAgent()\n",
    "player2 = PokerAgent()\n",
    "player3 = PokerAgent()\n",
    "\n",
    "for e in range(NUM_EPISODES):\n",
    "    print(\"Starting Episode: \" + str(e+1))\n",
    "    # Declare game setup paramers\n",
    "    config = setup_config(max_round=100, initial_stack=INITIAL_STACK, small_blind_amount=5)\n",
    "    config.register_player(name = 'p1', algorithm=player1)\n",
    "    config.register_player(name = 'p2', algorithm=player2)\n",
    "    config.register_player(name = 'p3', algorithm=player3)\n",
    "\n",
    "    # play poker game \n",
    "    \n",
    "    game_result = start_poker(config, verbose=0)\n",
    "\n",
    "    player1.train_long_mem()\n",
    "    player2.train_long_mem()\n",
    "\n",
    "    if e % 100 == 0:\n",
    "        player1.save_agent(\"./models/\" + model_file_name)\n",
    "\n",
    "player1.save_agent(\"./models/\" + model_file_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "import time\n",
    "\n",
    "def update_bar_chart(winnings, labels, game_num):\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.bar(labels, winnings, color='skyblue')\n",
    "\n",
    "    # Add labels and title\n",
    "    plt.xlabel('Winnings')\n",
    "    plt.ylabel('Values')\n",
    "    plt.title('Winnings After Game: ' + str(game_num))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "training_mode_on = False\n",
    "\n",
    "player1 = PokerAgent()\n",
    "player2 = RandomPlayer()\n",
    "player3 = HonestPlayer()\n",
    "\n",
    "\n",
    "# # game = []\n",
    "# # winpercent = []\n",
    "dan_wins = 0\n",
    "random_wins = 0\n",
    "honest_wins = 0\n",
    "labels = ['Daniel Negreanu', 'RandomBot', 'HonestBot']\n",
    "winnings = [0, 0, 0]\n",
    "for i in range(1, 100):\n",
    "    config = setup_config(max_round=100, initial_stack=INITIAL_STACK, small_blind_amount=5)\n",
    "    config.register_player(name=\"Daniel Negreanu\", algorithm=player1)\n",
    "    config.register_player(name=\"RandomBot\", algorithm=player2)\n",
    "    config.register_player(name=\"HonestBot\", algorithm=player3)\n",
    "    game_result = start_poker(config, verbose=0)\n",
    "    for player_info in game_result[\"players\"]:\n",
    "        if player_info['name'] == \"Daniel Negreanu\":\n",
    "            winnings[0] += (player_info['stack'] - INITIAL_STACK)\n",
    "        if player_info['name'] == \"HonestBot\":\n",
    "            winnings[2] += (player_info['stack'] - INITIAL_STACK)\n",
    "        if player_info['name'] == \"RandomBot\":\n",
    "            winnings[1] += (player_info['stack'] - INITIAL_STACK)\n",
    "    update_bar_chart(winnings, labels, i)\n",
    "    time.sleep(0.5)\n",
    "    clear_output(wait=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pkrenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "13cc0df5bbbf998b72edacc6b8887e6c4f34ef072e22ef9566e0b6f19315d46f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
