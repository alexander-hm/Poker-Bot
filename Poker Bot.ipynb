{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "265b80b8",
   "metadata": {},
   "source": [
    "# Poker Bot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c663df",
   "metadata": {},
   "source": [
    "Poker Bot built using TensorFlow. Trained with reinforcement learning using the PyPokerEngine library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1484e6c",
   "metadata": {},
   "source": [
    "## TO DO:\n",
    "### Q Learning\n",
    "Implement a Q learning training sytem to update the model after each round with the following:\n",
    "- Observation: Game state observed before action (hole_card, round_state)\n",
    "- Action: Output of declare_action\n",
    "- Reward: The net change in chips from the round\n",
    "\n",
    "Note that the multiple actions taken during a single round have the same reward but different game states so are independent phenomena. \n",
    "\n",
    "### Error: Raises\n",
    "When raising is an invalid move the valid_actions \"amount\" field says -1. When the model raises even when it is not able to it raises the pot to -1.\n",
    "- Need to make the model choose next best option if raise invalid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c383262",
   "metadata": {},
   "source": [
    "### Building the Poker AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51373e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages to install\n",
    "# pip install PyPokerEngine\n",
    "# pip install pyyaml h5py  # Required to save models in HDF5 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bd570406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import collections\n",
    "import collections.abc\n",
    "import h5py\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow.keras.layers import Dense, Flatten, Reshape, LeakyReLU\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import keras\n",
    "import keras.callbacks\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0808836",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypokerengine.players import BasePokerPlayer\n",
    "from pypokerengine.api.emulator import Emulator\n",
    "from pypokerengine.utils.game_state_utils import restore_game_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8679991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notes:\n",
    "\n",
    "# use tf.keras.callbacks.ModelCheckpoint to continually \n",
    "# save the model both during and at the end of training.\n",
    "# https://www.tensorflow.org/tutorials/keras/save_and_load"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f589aa6a",
   "metadata": {},
   "source": [
    "In this implementation, the feature vector's length is constant for every game state. The community cards are represented as 5 pairs of suit and rank features, with placeholders for missing cards. The action histories are aggregated into a fixed number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bb25ba",
   "metadata": {},
   "source": [
    "#### Model Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e3a66b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Program settings\n",
    "global DEBUG\n",
    "DEBUG = False\n",
    "\n",
    "# Reinforcement learning active\n",
    "global train_model\n",
    "train_model = True\n",
    "\n",
    "# Load saved model or create new\n",
    "global save_model\n",
    "global load_saved_model\n",
    "global num_saves\n",
    "global model_pathname\n",
    "save_model = False\n",
    "load_saved_model = False\n",
    "num_saves = 0\n",
    "model_pathname = 'pathname'\n",
    "\n",
    "# Model info\n",
    "global model_input_size\n",
    "model_input_size = 34"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5589477a",
   "metadata": {},
   "source": [
    "#### Q Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "8aa3df63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q(S, A)\n",
    "# Q value a function of the states and actions (game state and actions taken)\n",
    "# AKA how good it is to take action A at state S\n",
    "\n",
    "# TD-Update (Temporal Difference)\n",
    "# Q(S,A) <-- Q(S,A) + alpha * (R + gamma * Q(S',A') - Q(S,A))))\n",
    "\n",
    "# S :Current State of the agent.\n",
    "# A  : Current Action Picked according to some policy.\n",
    "# S'  : Next State where the agent ends up.\n",
    "# A'  : Next best action to be picked using current Q-value estimation, i.e. pick the action with the maximum Q-value in the next state.\n",
    "# R  : Current Reward observed from the environment in Response of current action.\n",
    "\n",
    "# (>0 and <=1) : Discounting factor for future rewards\n",
    "gamma = 0.2 \n",
    "\n",
    "# Step length taken to update the estimation of Q(S, A)\n",
    "alpha = 1\n",
    "\n",
    "# Greedy policy\n",
    "# Probability of choosing any action at random (vs. action with highest Q value)\n",
    "epsilon = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c0d66bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define neural network architecture\n",
    "def create_model(input_shape):\n",
    "    if load_saved_model and os.path.isfile(model_pathname):\n",
    "        model = model.load_model(model_pathname)\n",
    "    else:\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(128, activation='relu', input_shape=input_shape),\n",
    "            tf.keras.layers.Dropout(0.5),\n",
    "            tf.keras.layers.Dense(64, activation='relu'),\n",
    "            tf.keras.layers.Dense(32, activation='relu'),\n",
    "            tf.keras.layers.Dense(3, activation='softmax')  # Assuming simple output (fold, call, or raise)\n",
    "        ])\n",
    "        model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c95544df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper methods\n",
    "def flatten(x):\n",
    "        if isinstance(x, collections.Iterable):\n",
    "            return [a for i in x for a in flatten(i)]\n",
    "        else:\n",
    "            return [x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "fbb6f20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PokerBot(BasePokerPlayer):\n",
    "    \n",
    "    # TODO: Read some saved state of the model to allow reinforcement learning over time\n",
    "    def __init__(self):\n",
    "        # Initialize model\n",
    "        self.model = create_model((model_input_size,))  # Input shape, adjust based on features\n",
    "        # Save game states and actions in input output matrices for Q learning\n",
    "        self.x = [] # np.empty((0, model_input_size))\n",
    "        self.Y = [] # np.empty((0, 2)) # action, amount\n",
    "    \n",
    "    # TODO: Look at Emulator implementation to make the model work with reinforcement learning\n",
    "    # Make an action based on the model output\n",
    "    def declare_action(self, valid_actions, hole_card, round_state):\n",
    "        # Prepare feature vector based on the game state\n",
    "        feature_vector = self._extract_features(hole_card, round_state)\n",
    "        if DEBUG:\n",
    "            print(\"input size: \" + str(len(feature_vector)))\n",
    "            print(\"input shape: \" + str(feature_vector.shape))\n",
    "        \n",
    "        \n",
    "        # Use the model to predict the action\n",
    "        action_probs = self.model.predict(feature_vector).flatten()\n",
    "        # If raising invalid move restrict valid moves to fold and call\n",
    "        if valid_actions[2][\"amount\"][\"max\"] == -1:\n",
    "            action_probs = action_probs[:2]\n",
    "        outcome = np.argmax(action_probs)\n",
    "        \n",
    "        if DEBUG:\n",
    "            print(\"action_probs length: \" + str(len(action_probs)))\n",
    "            print(action_probs)\n",
    "            print(\"action argmax: \" + str(outcome))\n",
    "        \n",
    "        action_info = valid_actions[outcome]\n",
    "        action = action_info[\"action\"]\n",
    "        if outcome == 2:\n",
    "            # Scale raise to the confidence of the model\n",
    "            amount = action_info[\"amount\"][\"min\"] + math.floor((action_info[\"amount\"][\"max\"] - action_info[\"amount\"][\"min\"]) * action_probs[outcome])\n",
    "            if DEBUG:\n",
    "                print(valid_actions)\n",
    "                print(str((action_info[\"amount\"][\"max\"] - action_info[\"amount\"][\"min\"]) * action_probs[outcome]))\n",
    "        else:\n",
    "            amount = action_info[\"amount\"]\n",
    "        \n",
    "        # Update Q learning input, output (observation, action)\n",
    "        # np.append(self.x, feature_vector, axis=1)\n",
    "        # np.append(self.Y, [action, amount], axis=1)\n",
    "        self.x.append(feature_vector)\n",
    "        self.Y.append([action, amount])\n",
    "        \n",
    "        return action, amount\n",
    "    \n",
    "    # Setup Emulator object by registering game information\n",
    "    def receive_game_start_message(self, game_info):\n",
    "        return\n",
    "        \n",
    "        # Emulator skeleton code\n",
    "        player_num = game_info[\"player_num\"]\n",
    "        max_round = game_info[\"rule\"][\"max_round\"]\n",
    "        small_blind_amount = game_info[\"rule\"][\"small_blind_amount\"]\n",
    "        ante_amount = game_info[\"rule\"][\"ante\"]\n",
    "        blind_structure = game_info[\"rule\"][\"blind_structure\"]\n",
    "        \n",
    "        self.emulator = Emulator()\n",
    "        self.emulator.set_game_rule(player_num, max_round, small_blind_amount, ante_amount)\n",
    "        self.emulator.set_blind_structure(blind_structure)\n",
    "        \n",
    "        # Register algorithm of each player which used in the simulation.\n",
    "        for player_info in game_info[\"seats\"][\"players\"]:\n",
    "            self.emulator.register_player(player_info[\"uuid\"], PokerBot())\n",
    "\n",
    "    # Not neccesarily useful\n",
    "    def receive_round_start_message(self, round_count, hole_card, seats):\n",
    "        # Reset Round info for Q learning\n",
    "        # NOTE: Only for debugging purposes. Optimal practices may vary\n",
    "        self.x = []\n",
    "        self.Y = []\n",
    "        pass\n",
    "\n",
    "    # Not neccesarily useful\n",
    "    def receive_street_start_message(self, street, round_state):\n",
    "        pass\n",
    "\n",
    "    # Can incorporate player observation in model updated with each move\n",
    "    def receive_game_update_message(self, new_action, round_state):\n",
    "        pass\n",
    "    \n",
    "    # Update model with each round result\n",
    "    def receive_round_result_message(self, winners, hand_info, round_state):\n",
    "        if train_model:\n",
    "            # Update model with round results\n",
    "            pass\n",
    "        if save_model:\n",
    "            # Save model to file\n",
    "            self.model.save(model_pathname + \"_V\" + num_saves)\n",
    "        \n",
    "        # print(self.x)\n",
    "        print(\"Round actions: player \" + self.uuid)\n",
    "        self._print_hole_cards(self.x)\n",
    "        print(self.Y)\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    # Additional methods\n",
    "    \n",
    "    # Produce a feature vector of length 34\n",
    "    def _extract_features(self, hole_card, round_state):\n",
    "        # 4 Features from hole cards\n",
    "        hole_card_features = [self._card_to_feature(card) for card in hole_card]\n",
    "\n",
    "        # 10 Features from community cards (always represent 5 cards)\n",
    "        community_cards = round_state['community_card'] + [None] * (5 - len(round_state['community_card']))\n",
    "        community_card_features = [self._card_to_feature(card) if card else [0, 0] for card in community_cards]\n",
    "\n",
    "        # 8 Standard features\n",
    "        standard_features = [\n",
    "            round_state['round_count'],\n",
    "            round_state['pot']['main']['amount'],\n",
    "            sum([side_pot['amount'] for side_pot in round_state['pot']['side']]),\n",
    "            round_state['dealer_btn'],\n",
    "            round_state['small_blind_pos'],\n",
    "            round_state['big_blind_pos'],\n",
    "            round_state['small_blind_amount'],\n",
    "            self._street_to_feature(round_state['street'])\n",
    "        ]\n",
    "\n",
    "        # 8 Action history features (2 {# raises, # calls} for each betting stage: preflop, flop, turn, river)\n",
    "        action_history_features = self._aggregate_action_histories(round_state['action_histories'])\n",
    "\n",
    "        # Combine all features into a single fixed-size feature vector of length 34\n",
    "        # Flatten the list of lists\n",
    "        features = flatten(hole_card_features + community_card_features + standard_features + action_history_features)\n",
    "        features = np.array(features)\n",
    "        features = features.reshape(1, -1)\n",
    "        return features\n",
    "    \n",
    "    def _card_to_feature(self, card):\n",
    "        # Convert card to a numerical feature\n",
    "        suits = {'C': 1, 'D': 2, 'H': 3, 'S': 4, 'None': 0}\n",
    "        ranks = {'2': 2, '3': 3, '4': 4, '5': 5, '6': 6, '7': 7, '8': 8, '9': 9, 'T': 10, 'J': 11, 'Q': 12, 'K': 13, 'A': 14, 'None': 0}\n",
    "        suit = suits.get(card[0], 0) if card else 0\n",
    "        rank = ranks.get(card[1], 0) if card else 0\n",
    "        return [suit, rank]\n",
    "    \n",
    "    def _feature_to_card(self, card):\n",
    "        # Reverse mappings\n",
    "        suits_reverse = {1: 'C', 2: 'D', 3: 'H', 4: 'S', 0: 'None'}\n",
    "        ranks_reverse = {2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9', 10: 'T', 11: 'J', 12: 'Q', 13: 'K', 14: 'A', 0: 'None'}\n",
    "        \n",
    "        # Map features back to card representation\n",
    "        suit = suits_reverse.get(int(card[0]), 'None')\n",
    "        rank = ranks_reverse.get(int(card[1]), 'None')\n",
    "\n",
    "        # Combine suit and rank to form card representation\n",
    "        # If either suit or rank is 'None', the card is considered invalid or not present\n",
    "        if suit == 'None' or rank == 'None':\n",
    "            return 'None'\n",
    "        else:\n",
    "            return suit + rank\n",
    "    \n",
    "    def _street_to_feature(self, street):\n",
    "        # Convert street to a numerical feature\n",
    "        streets = {'preflop': 1, 'flop': 2, 'turn': 3, 'river': 4, 'showdown': 5}\n",
    "        return streets.get(street, 0)\n",
    "\n",
    "    def _aggregate_action_histories(self, action_histories):\n",
    "        '''\n",
    "        # Aggregate action histories into a fixed-length vector\n",
    "        # Example: Count the number of raises, calls, etc.\n",
    "        raise_count = sum(1 for action in action_histories.get('preflop', []) if action['action'] == 'raise')\n",
    "        call_count = sum(1 for action in action_histories.get('preflop', []) if action['action'] == 'call')\n",
    "        # Add more aggregated features as needed\n",
    "        # Ensure the length of this vector is fixed\n",
    "        return [raise_count, call_count]\n",
    "        '''\n",
    "        \n",
    "        # Initialize counts\n",
    "        raise_count = [0, 0, 0, 0]  # Preflop, Flop, Turn, River\n",
    "        call_count = [0, 0, 0, 0]\n",
    "        fold_count = [0, 0, 0, 0]\n",
    "\n",
    "        # Define rounds\n",
    "        rounds = ['preflop', 'flop', 'turn', 'river']\n",
    "\n",
    "        # Count actions in each round\n",
    "        for i, round in enumerate(rounds):\n",
    "            for action in action_histories.get(round, []):\n",
    "                if action['action'] == 'raise':\n",
    "                    raise_count[i] += 1\n",
    "                elif action['action'] == 'call':\n",
    "                    call_count[i] += 1\n",
    "                elif action['action'] == 'fold':\n",
    "                    fold_count[i] += 1\n",
    "\n",
    "        # Flatten and return\n",
    "        return raise_count + call_count + fold_count\n",
    "    \n",
    "    def _print_hole_cards(self, feature_list):\n",
    "        for features in feature_list:\n",
    "            # Assuming the first four features are for the two hole cards\n",
    "            card1_features = features[0][0:2]  # First two features for the first card\n",
    "            card2_features = features[0][2:4]  # Next two features for the second card\n",
    "            \n",
    "            # Convert features to card representations\n",
    "            card1 = self._feature_to_card(card1_features)\n",
    "            card2 = self._feature_to_card(card2_features)\n",
    "\n",
    "            # Print the hole cards for this round\n",
    "            print(f\"Hole Cards: {card1}, {card2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1091405",
   "metadata": {},
   "source": [
    "### Simulating Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "36a9de4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started the round 1\n",
      "Street \"preflop\" started. (community card = [])\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "\"p1\" declared \"raise:89\"\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "\"p2\" declared \"call:89\"\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "\"p3\" declared \"fold:0\"\n",
      "Street \"flop\" started. (community card = ['H8', 'HT', 'C9'])\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "\"p2\" declared \"call:0\"\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "\"p1\" declared \"raise:10\"\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "\"p2\" declared \"call:10\"\n",
      "Street \"turn\" started. (community card = ['H8', 'HT', 'C9', 'H7'])\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "\"p2\" declared \"call:0\"\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "\"p1\" declared \"fold:0\"\n",
      "\"['p2']\" won the round 1 (stack = {'p1': 1, 'p2': 209, 'p3': 90})\n",
      "Round actions: player hdagfeczuamlwbaxajkgvm\n",
      "Hole Cards: C7, D9\n",
      "Hole Cards: C7, D9\n",
      "Hole Cards: C7, D9\n",
      "[['raise', 89], ['raise', 10], ['fold', 0]]\n",
      "Round actions: player mhvwceygwkyxlvnothorxl\n",
      "Hole Cards: CA, DT\n",
      "Hole Cards: CA, DT\n",
      "Hole Cards: CA, DT\n",
      "Hole Cards: CA, DT\n",
      "[['call', 89], ['call', 0], ['call', 10], ['call', 0]]\n",
      "Round actions: player ptvagydvgxrtffupjlllpc\n",
      "Hole Cards: C8, S5\n",
      "[['fold', 0]]\n",
      "Started the round 2\n",
      "Street \"preflop\" started. (community card = [])\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\"p3\" declared \"fold:0\"\n",
      "\"['p2']\" won the round 2 (stack = {'p1': 0, 'p2': 214, 'p3': 85})\n",
      "Round actions: player hdagfeczuamlwbaxajkgvm\n",
      "[]\n",
      "Round actions: player mhvwceygwkyxlvnothorxl\n",
      "[]\n",
      "Round actions: player ptvagydvgxrtffupjlllpc\n",
      "Hole Cards: H9, CT\n",
      "[['fold', 0]]\n",
      "Started the round 3\n",
      "Street \"preflop\" started. (community card = [])\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "\"p2\" declared \"call:10\"\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "\"p3\" declared \"fold:0\"\n",
      "\"['p2']\" won the round 3 (stack = {'p1': 0, 'p2': 224, 'p3': 75})\n",
      "Round actions: player hdagfeczuamlwbaxajkgvm\n",
      "[]\n",
      "Round actions: player mhvwceygwkyxlvnothorxl\n",
      "Hole Cards: CK, C9\n",
      "[['call', 10]]\n",
      "Round actions: player ptvagydvgxrtffupjlllpc\n",
      "Hole Cards: C5, D7\n",
      "[['fold', 0]]\n",
      "Started the round 4\n",
      "Street \"preflop\" started. (community card = [])\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "\"p3\" declared \"fold:0\"\n",
      "\"['p2']\" won the round 4 (stack = {'p1': 0, 'p2': 229, 'p3': 70})\n",
      "Round actions: player hdagfeczuamlwbaxajkgvm\n",
      "[]\n",
      "Round actions: player mhvwceygwkyxlvnothorxl\n",
      "[]\n",
      "Round actions: player ptvagydvgxrtffupjlllpc\n",
      "Hole Cards: DQ, C9\n",
      "[['fold', 0]]\n",
      "Started the round 5\n",
      "Street \"preflop\" started. (community card = [])\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "\"p2\" declared \"call:10\"\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "\"p3\" declared \"fold:0\"\n",
      "\"['p2']\" won the round 5 (stack = {'p1': 0, 'p2': 239, 'p3': 60})\n",
      "Round actions: player hdagfeczuamlwbaxajkgvm\n",
      "[]\n",
      "Round actions: player mhvwceygwkyxlvnothorxl\n",
      "Hole Cards: C6, C3\n",
      "[['call', 10]]\n",
      "Round actions: player ptvagydvgxrtffupjlllpc\n",
      "Hole Cards: S7, C7\n",
      "[['fold', 0]]\n",
      "Started the round 6\n",
      "Street \"preflop\" started. (community card = [])\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "\"p3\" declared \"fold:0\"\n",
      "\"['p2']\" won the round 6 (stack = {'p1': 0, 'p2': 244, 'p3': 55})\n",
      "Round actions: player hdagfeczuamlwbaxajkgvm\n",
      "[]\n",
      "Round actions: player mhvwceygwkyxlvnothorxl\n",
      "[]\n",
      "Round actions: player ptvagydvgxrtffupjlllpc\n",
      "Hole Cards: C4, S2\n",
      "[['fold', 0]]\n",
      "Started the round 7\n",
      "Street \"preflop\" started. (community card = [])\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "\"p2\" declared \"call:10\"\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "\"p3\" declared \"fold:0\"\n",
      "\"['p2']\" won the round 7 (stack = {'p1': 0, 'p2': 254, 'p3': 45})\n",
      "Round actions: player hdagfeczuamlwbaxajkgvm\n",
      "[]\n",
      "Round actions: player mhvwceygwkyxlvnothorxl\n",
      "Hole Cards: DT, C3\n",
      "[['call', 10]]\n",
      "Round actions: player ptvagydvgxrtffupjlllpc\n",
      "Hole Cards: H8, D8\n",
      "[['fold', 0]]\n",
      "Started the round 8\n",
      "Street \"preflop\" started. (community card = [])\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "\"p3\" declared \"fold:0\"\n",
      "\"['p2']\" won the round 8 (stack = {'p1': 0, 'p2': 259, 'p3': 40})\n",
      "Round actions: player hdagfeczuamlwbaxajkgvm\n",
      "[]\n",
      "Round actions: player mhvwceygwkyxlvnothorxl\n",
      "[]\n",
      "Round actions: player ptvagydvgxrtffupjlllpc\n",
      "Hole Cards: DQ, D3\n",
      "[['fold', 0]]\n",
      "Started the round 9\n",
      "Street \"preflop\" started. (community card = [])\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "\"p2\" declared \"call:10\"\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "\"p3\" declared \"fold:0\"\n",
      "\"['p2']\" won the round 9 (stack = {'p1': 0, 'p2': 269, 'p3': 30})\n",
      "Round actions: player hdagfeczuamlwbaxajkgvm\n",
      "[]\n",
      "Round actions: player mhvwceygwkyxlvnothorxl\n",
      "Hole Cards: CJ, S4\n",
      "[['call', 10]]\n",
      "Round actions: player ptvagydvgxrtffupjlllpc\n",
      "Hole Cards: D2, CQ\n",
      "[['fold', 0]]\n",
      "Started the round 10\n",
      "Street \"preflop\" started. (community card = [])\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "\"p3\" declared \"fold:0\"\n",
      "\"['p2']\" won the round 10 (stack = {'p1': 0, 'p2': 274, 'p3': 25})\n",
      "Round actions: player hdagfeczuamlwbaxajkgvm\n",
      "[]\n",
      "Round actions: player mhvwceygwkyxlvnothorxl\n",
      "[]\n",
      "Round actions: player ptvagydvgxrtffupjlllpc\n",
      "Hole Cards: D7, D6\n",
      "[['fold', 0]]\n"
     ]
    }
   ],
   "source": [
    "from pypokerengine.api.game import setup_config, start_poker\n",
    "\n",
    "# Declare game setup paramers\n",
    "config = setup_config(max_round=10, initial_stack=100, small_blind_amount=5)\n",
    "config.register_player(name=\"p1\", algorithm=PokerBot())\n",
    "config.register_player(name=\"p2\", algorithm=PokerBot())\n",
    "config.register_player(name=\"p3\", algorithm=PokerBot())\n",
    "game_result = start_poker(config, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5dd923",
   "metadata": {},
   "source": [
    "### Save Model to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c3c36e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
