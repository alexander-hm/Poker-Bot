{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import collections\n",
    "from tensorflow import keras\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "import keras\n",
    "import keras.callbacks\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypokerengine.players import BasePokerPlayer\n",
    "from pypokerengine.api.emulator import Emulator\n",
    "from pypokerengine.utils.game_state_utils import restore_game_state\n",
    "from pypokerengine.utils.card_utils import estimate_hole_card_win_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Program settings\n",
    "global DEBUG\n",
    "DEBUG = False\n",
    "global WATCH_GAME\n",
    "WATCH_GAME = True\n",
    "\n",
    "\n",
    "# Load saved model or create new\n",
    "global save_model_to_file\n",
    "global load_saved_model\n",
    "global num_saves\n",
    "global model_pathname\n",
    "save_model_to_file = False\n",
    "load_saved_model = False\n",
    "num_saves = 0\n",
    "model_pathname = 'pathname'\n",
    "\n",
    "# Model info\n",
    "global model_input_size\n",
    "model_input_size = 21\n",
    "model_output_size = 4\n",
    "MEMORY_SIZE = 10000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "#Training Info\n",
    "NUM_EPISODES = 10000\n",
    "TARGET_LAG_FACTOR = 7500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep RL constrants\n",
    "gamma = 0.2 \n",
    "\n",
    "# Step length taken to update the estimation of Q(S, A)\n",
    "alpha = 1\n",
    "\n",
    "# Greedy policy\n",
    "# Probability of choosing any action at random (vs. action with highest Q value)\n",
    "epsilon = 0.1\n",
    "epsilon_min = 0.05\n",
    "epsilon_decay = 0.99\n",
    "\n",
    "\n",
    "# Target Model Ketchup\n",
    "target_n_val = 7500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNetwork:\n",
    "    def __init__(self, input_size, output_size):\n",
    "\n",
    "        # model\n",
    "        super(DQNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, output_size)\n",
    "\n",
    "        self.target_model = DQNetwork(model_input_size, model_output_size)\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        self.target_model.eval()\n",
    "\n",
    "        # set optimizer and loss functions for models\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n",
    "        self.criterion = nn.MSELoss()\n",
    "    \n",
    "    def model_forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "    def train_step(self, state, action, reward, next_state, done):\n",
    "        state = torch.tensor(state, dtype=torch.float32)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float32)\n",
    "        action = torch.tensor(action, dtype=torch.float32)\n",
    "        reward = torch.tensor(reward, dtype=torch.float32)\n",
    "\n",
    "        if len(state.shape) == 1:\n",
    "            state = torch.unsqueeze(state, 0)\n",
    "            next_state = torch.unsqueeze(next_state, 0)\n",
    "            action = torch.unsqueeze(action, 0)\n",
    "            reward = torch.unsqueeze(reward)\n",
    "            done = (done, )\n",
    "\n",
    "        q_values = self.model_forward(state)\n",
    "        next_q_values = self.model_forward(next_state)\n",
    "\n",
    "        target = reward + gamma * torch.max(next_q_values) * (1 - done)\n",
    "\n",
    "        loss = self.criterion(target, q_values)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Decay exploration rate\n",
    "        if epsilon > epsilon_min:\n",
    "            epsilon *= epsilon_decay\n",
    "        \n",
    "\n",
    "    def save(self, file_name):\n",
    "        file_path = \"./models\" + file_name\n",
    "        torch.save(self.state_dict(), file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PokerAgent:\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        # Initialize model and target model\n",
    "        if load_saved_model:\n",
    "            self.model = torch.load(model_pathname)\n",
    "        else:\n",
    "            self.model = DQNetwork(model_input_size, model_output_size)\n",
    "\n",
    "        # set optimizer and loss functions for model\n",
    "\n",
    "        # Experience replay\n",
    "        self.memory = collections.deque(maxlen = MEMORY_SIZE)\n",
    "        self.batch_size = BATCH_SIZE\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            return random.randint(0, model_output_size - 1)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.tensor(state, dtype=torch.float32).view(1, -1)\n",
    "                q_values = self.model.predict(state)\n",
    "                return torch.argmax(q_values).item()\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def train_short_mem(self, state, action, reward, next_state, done):\n",
    "        self.model.train_step (self, state, action, reward, next_state, done)\n",
    "\n",
    "    def train_long_mem(self, state, action, reward, next_state, done):\n",
    "        sample = None\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            sample = random.sample(self.memory, BATCH_SIZE)\n",
    "        else:\n",
    "            sample = self.memory\n",
    "        states, actions, rewards, next_states, dones = zip(*sample)\n",
    "        model = self.model.train_step(states, actions, rewards, next_states, dones)\n",
    "        \n",
    "\n",
    "    def declare_action(self, valid_actions, hole_card, round_state):\n",
    "        # Prepare feature vector based on the game state\n",
    "        feature_vector = self._extract_features(hole_card, round_state)\n",
    "\n",
    "\n",
    "        if DEBUG:\n",
    "            print(\"input size: \" + str(len(feature_vector)))\n",
    "            print(\"input shape: \" + str(feature_vector.shape))\n",
    "        \n",
    "        \n",
    "        # Use the model to predict the action\n",
    "        # action will be number 1-4\n",
    "        ## 1 -> fold\n",
    "        ## 2 -> call\n",
    "        ## 3 -> min raise\n",
    "        ## 4 -> max raise\n",
    "        action_num = self.select_action(feature_vector)\n",
    "\n",
    "        action_map = {0: 'fold', 1: 'call', 2: 'raise', 3: 'raise'}\n",
    "\n",
    "        action = action_map.get(action_num, 0)\n",
    "        amount = 0\n",
    "\n",
    "        #get call val\n",
    "        if action_num == 1:\n",
    "            amount = valid_actions[1]['amount']\n",
    "\n",
    "        # get min raise val\n",
    "        if action_num == 2:\n",
    "            amount = valid_actions[2]['amount']['min']\n",
    "        \n",
    "        # get max raise val\n",
    "        if action_num == 3:\n",
    "            amount = valid_actions[2]['amount']['min']\n",
    "        \n",
    "        \n",
    "        \n",
    "        return action, amount\n",
    "    \n",
    "    def receive_game_start_message(self, game_info):\n",
    "    \n",
    "        player_num = game_info['player_num']\n",
    "        max_round = game_info['rule']['max_round']\n",
    "        small_blind_amount = game_info['rule']['small_blind_amount']\n",
    "        ante_amount = game_info['rule']['ante']\n",
    "        blind_structure = game_info['rule']['blind_structure']\n",
    "    \n",
    "    def _extract_features(self, hole_card, round_state):\n",
    "        \n",
    "        #simulate hand against 10000 flops extracting hand strength estimate\n",
    "        \n",
    "        hand_strength = self._hand_strength_sim(hole_card, round_state['community_card'])\n",
    "\n",
    "        # 8 Standard features\n",
    "        standard_features = [\n",
    "            round_state['round_count'],\n",
    "            round_state['pot']['main']['amount'],\n",
    "            sum([side_pot['amount'] for side_pot in round_state['pot']['side']]),\n",
    "            round_state['dealer_btn'],\n",
    "            round_state['small_blind_pos'],\n",
    "            round_state['big_blind_pos'],\n",
    "            round_state['small_blind_amount'],\n",
    "            self._street_to_feature(round_state['street'])\n",
    "        ]\n",
    "\n",
    "        # 8 Action history features (2 {# raises, # calls} for each betting stage: preflop, flop, turn, river)\n",
    "        action_history_features = self._aggregate_action_histories(round_state['action_histories'])\n",
    "\n",
    "        # Combine all features into a single fixed-size feature vector of length 34\n",
    "        # Flatten the list of lists\n",
    "        features = flatten([hand_strength] + standard_features + action_history_features)\n",
    "        features = np.array(features)\n",
    "        features = features.reshape(1, -1)\n",
    "        return features\n",
    "    \n",
    "    # Not neccesarily useful\n",
    "    def receive_round_start_message(self, round_count, hole_card, seats):\n",
    "        pass\n",
    "\n",
    "    # Not neccesarily useful\n",
    "    def receive_street_start_message(self, street, round_state):\n",
    "        pass\n",
    "\n",
    "    # Can incorporate player observation in model updated with each move\n",
    "    def receive_game_update_message(self, new_action, round_state):\n",
    "        pass\n",
    "    \n",
    "    def receive_round_result_message(self, winners, hand_info, round_state):\n",
    "        # Calculate net chip gain from round\n",
    "        if winners[0]['uuid'] == self.uuid:\n",
    "            # Player won the round\n",
    "            print(\"Player \", winners[0]['uuid'], \" won the round\")\n",
    "            gain = 1\n",
    "        \n",
    "        num_rounds += 1\n",
    "        if num_rounds % TARGET_LAG_FACTOR == 0:\n",
    "                \n",
    "        \n",
    "        if WATCH_GAME:\n",
    "            print(\"Round actions: player \" + self.uuid)\n",
    "            print(\"Y: \", self.Y)\n",
    "            print(\"Q: \", self.Q)\n",
    "        \n",
    "        # Update model\n",
    "        if train_model:\n",
    "            # Update model with round results\n",
    "            pass\n",
    "        if save_model_to_file:\n",
    "            # Save model to file\n",
    "            save_model()\n",
    "        pass\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper methods\n",
    "def flatten(x):\n",
    "        if isinstance(x, collections.Iterable):\n",
    "            return [a for i in x for a in flatten(i)]\n",
    "        else:\n",
    "            return [x]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PokerAgent' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Declare game setup paramers\u001b[39;00m\n\u001b[1;32m      4\u001b[0m config \u001b[39m=\u001b[39m setup_config(max_round\u001b[39m=\u001b[39m\u001b[39m200000\u001b[39m, initial_stack\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, small_blind_amount\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m config\u001b[39m.\u001b[39mregister_player(name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mp1\u001b[39m\u001b[39m\"\u001b[39m, algorithm\u001b[39m=\u001b[39mPokerBot())\n\u001b[1;32m      6\u001b[0m config\u001b[39m.\u001b[39mregister_player(name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mp2\u001b[39m\u001b[39m\"\u001b[39m, algorithm\u001b[39m=\u001b[39mPokerBot())\n\u001b[1;32m      7\u001b[0m config\u001b[39m.\u001b[39mregister_player(name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mp3\u001b[39m\u001b[39m\"\u001b[39m, algorithm\u001b[39m=\u001b[39mPokerBot())\n",
      "Cell \u001b[0;32mIn[103], line 9\u001b[0m, in \u001b[0;36mPokerBot.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      7\u001b[0m     load_model()\n\u001b[1;32m      8\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m----> 9\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m PokerAgent()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'PokerAgent' is not defined"
     ]
    }
   ],
   "source": [
    "from pypokerengine.api.game import setup_config, start_poker\n",
    "\n",
    "#creat models to train\n",
    "\n",
    "player1 = PokerAgent()\n",
    "player2 = PokerAgent()\n",
    "player3 = PokerAgent()\n",
    "\n",
    "for e in range(NUM_EPISODES):\n",
    "\n",
    "    # Declare game setup paramers\n",
    "    config = setup_config(max_round=1000, initial_stack=100, small_blind_amount=5)\n",
    "    config.register_player(name = 'p1', algorithm=player1)\n",
    "    config.register_player(name = 'p2', algorithm=player2)\n",
    "    config.register_player(name = 'p3', algorithm=player3)\n",
    "\n",
    "    # play poker game \n",
    "    \n",
    "    game_result = start_poker(config, verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pkrenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "13cc0df5bbbf998b72edacc6b8887e6c4f34ef072e22ef9566e0b6f19315d46f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
